{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynPRbJviL-39"
   },
   "source": [
    "# NLP with Bert for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z82c-Fcay0a3"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we have imported the os.path library which will help us to add the path of the dataset. We also imported tensorflow which is tensorflow 2.0 and ktrain library which will help us to build the BERT model. Numpy library was also imported. We specifically imported the text module from the ktrain library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEjlM2EazOf0"
   },
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePywR8A4zaxT"
   },
   "source": [
    "### Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 56s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data = tf.keras.utils.get_file(fname = \"aclImdb_v1.tar.gz\",\n",
    "                              origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "                              extract = True)\n",
    "\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(data), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\.keras\\datasets\n",
      "C:\\Users\\User1\\.keras\\datasets\\aclImdb\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(data))\n",
    "print(IMDB_DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset directly from the AI Standford website (It contains a lot of great datasets). We use the get_file function from the tensorflow/keras library and the utils module. We are downloading the data directly from the official Stanford Website for AI. The above code is a short procedure to download it. We create a IMDB directory where we join the path of our keras library and the data from the website. I have also printed out the location of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8xLaiKHhW-z"
   },
   "source": [
    "### Creating the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder(datadir=IMDB_DATADIR,\n",
    "                                                                      classes=['pos','neg'],\n",
    "                                                                      maxlen=500,\n",
    "                                                                      train_test_names=['train','test'],\n",
    "                                                                      preprocess_mode='bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we are using a function called 'texts_from_folder' from the text module in the ktrain library. This allows us to get three variables which are the training set, testing set and the preprocessing of the texts.\n",
    "\n",
    "In this the parameters mentioned are datadir which should be the directory where all the texts are present. The classes within the library which in this case is positive and negative (Check the folder name). The maxlen specifies the number of characters you want from the text. The train_test_names should contain the folders name in which you have stored your training set texts and the testing set texts. The preprocess_mode should be bert in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bni9P0B0hpu8"
   },
   "source": [
    "## Part 2: Building the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "bert_model = text.text_classifier(name='bert',\n",
    "                                 train_data=(x_train, y_train),\n",
    "                                 preproc=preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we have created a BERT model using the text module from ktrain library. Ktrain library is a wrapper for the tensorflow and keras library. It make the execultion of models like BERT which are very complicated easy enough to be done in 1 line. We use the text_classifier function which using the method for classification as BERT, the tuple of the training and the test set and also the preprocessing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aa8pQ8-bhx0Z"
   },
   "source": [
    "## Part 3: Training the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model=bert_model,\n",
    "                            train_data=(x_train, y_train),\n",
    "                            val_data=(x_test, y_test),\n",
    "                            batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a learner part for the BERT model and classify it as positive or negative text. We use a function get_learner which is directly inside the ktrain library. It return a learner instance. It takes model as a paremeter and also the training and testing data and also the batch_size for length of test 500 the batch size is 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_onecycle(lr=2e-5, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call a function called the fit_onecycle from the get_learner function which is inside the ktrain library. We will train our BERT model only using 1 epoch. It feels it won't provide the high accuracy but it does!!!!!. We input the learning rate based on the number of epochs. We choose the number of epochs as 1. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP with Bert for Sentiment Analysis",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
