{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynPRbJviL-39"
   },
   "source": [
    "# NLP with Bert for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z82c-Fcay0a3"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we have imported the os.path library which will help us to add the path of the dataset. We also imported tensorflow which is tensorflow 2.0 and ktrain library which will help us to build the BERT model. Numpy library was also imported. We specifically imported the text module from the ktrain library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEjlM2EazOf0"
   },
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePywR8A4zaxT"
   },
   "source": [
    "### Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 56s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data = tf.keras.utils.get_file(fname = \"aclImdb_v1.tar.gz\",\n",
    "                              origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "                              extract = True)\n",
    "\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(data), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\.keras\\datasets\n",
      "C:\\Users\\User1\\.keras\\datasets\\aclImdb\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(data))\n",
    "print(IMDB_DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset directly from the AI Standford website (It contains a lot of great datasets). We use the get_file function from the tensorflow/keras library and the utils module. We are downloading the data directly from the official Stanford Website for AI. The above code is a short procedure to download it. We create a IMDB directory where we join the path of our keras library and the data from the website. I have also printed out the location of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8xLaiKHhW-z"
   },
   "source": [
    "### Creating the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bni9P0B0hpu8"
   },
   "source": [
    "## Part 2: Building the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aa8pQ8-bhx0Z"
   },
   "source": [
    "## Part 3: Training the BERT model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP with Bert for Sentiment Analysis",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
